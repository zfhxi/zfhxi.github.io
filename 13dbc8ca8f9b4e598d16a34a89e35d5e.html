<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <!-- iOS Safari -->
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <!-- Chrome, Firefox OS and Opera Status Bar Color -->
  <meta name="theme-color" content="#FFFFFF">
  <meta property="og:title" content="行为识别数据集简记">
    <meta property="og:type" content="blog">
  <title>行为识别数据集简记</title>
  <!-- Favicon -->
    <link rel="shortcut icon" href="🏡">
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css">
  <link rel="stylesheet" type="text/css"
    href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.19.0/themes/prism-solarizedlight.min.css">
  <link rel="stylesheet" type="text/css" href="css/SourceSansPro.css">
  <link rel="stylesheet" type="text/css" href="css/notablog.css">
  <link rel="stylesheet" type="text/css" href="css/theme.css">
  <script src="https://cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src='https://unpkg.com/valine/dist/Valine.min.js'></script>
  <style>
    :root {
      font-size: 18px;
    }

    .DateTagBar {
      margin-top: 1.0rem;
    }
  </style>
</head>

<body>
  <nav class="Navbar">
    <a href="index.html">
      <div class="Navbar__Btn"><span>🏡</span> <span>Home</span></div>
    </a>
                                                                                                                                                                                                                    <span class="Navbar__Delim">&centerdot;</span>
    <a href="links.html">
      <div class="Navbar__Btn"><span>🔗</span> <span>Links</span></div>
    </a>
                <span class="Navbar__Delim">&centerdot;</span>
    <a href="about.html">
      <div class="Navbar__Btn"><span>👦🏻</span> <span>About</span></div>
    </a>
                          </nav>
  <header class="Header">
        <div class="Header__Spacer Header__Spacer--NoCover">
    </div>
    <div>
            <span class="Header__Icon"><span>📄</span></span>
            <span class="Header__Title">行为识别数据集简记</span>

    </div>
        <div class="DateTagBar">
            <span class="DateTagBar__Item DateTagBar__Date">Posted on Wed, Sep 9, 2020</span>
                  <span class="DateTagBar__Item DateTagBar__Tag DateTagBar__Tag--green">
        <a href="tag/CV.html">CV</a>
      </span>
            <span class="DateTagBar__Item DateTagBar__Tag DateTagBar__Tag--orange">
        <a href="tag/Action Recognition.html">Action Recognition</a>
      </span>
          </div>
      </header>
  <article id="https://www.notion.so/13dbc8ca8f9b4e598d16a34a89e35d5e" class="PageRoot PageRoot--FullWidth"><h2 id="https://www.notion.so/d46f720cf00b4ed6b286e46e405b4682" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/d46f720cf00b4ed6b286e46e405b4682"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">20200909</span></span></h2><div id="https://www.notion.so/ff036f85434641b78a253919025bb3cb" class="CollectionInline"><h3><a class="Anchor" href="#https://www.notion.so/ff036f85434641b78a253919025bb3cb"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">各种数据集</span></span></h3><div class="Table"><table><thead><tr><th style="width:145px">DataSets</th><th style="width:376px">Introduction</th><th style="width:100px">Total Size</th><th style="width:177px">More Info</th><th style="width:147px">May be useful</th><th style="width:200px">Top</th></tr></thead><tbody><tr><td class="Table__CellTitle"><a href="https://www.notion.so/19d472a3ae57440c9bd6e7bf6552726e"><span class="SemanticStringArray"><span class="SemanticString">20BN-something-something V1</span></span></a></td><td class="Table__CellText"><span class="SemanticStringArray"><span class="SemanticString">a large collection of densely-labeled video clips that show humans performing pre-defined basic actions with everyday objects. The dataset was created by a large number of crowd workers. It allows machine learning models to develop</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold"> fine-grained</strong></span><span class="SemanticString"> understanding of basic actions that occur in the physical world.</span></span></td><td class="Table__CellText"><span class="SemanticStringArray"><span class="SemanticString">25.2GB</span></span></td><td class="Table__CellText"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://20bn.com/datasets/something-something/v1">https://20bn.com/datasets/something-something/v1</a></span></span></td><td class="Table__CellText"><span class="SemanticStringArray"><span class="SemanticString">人类对日常对象（剪刀、杯子等等）执行预定义的基本动作
</span></span></td><td class="Table__CellText"><span class="SemanticStringArray"><span class="SemanticString">54.01%(RGB-only 16f+8f (16611_10300))
50.72%(</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">TSM ResNet-50</strong></span><span class="SemanticString"> 8f, RGB+Flow)
40.71%(</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">2stream TRN</strong></span><span class="SemanticString">)
17.28%(</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">TSN</strong></span><span class="SemanticString">)</span></span></td></tr><tr><td class="Table__CellTitle"><a href="https://www.notion.so/f57fd777e0eb495c8b77c42cd2595c15"><span class="SemanticStringArray"><span class="SemanticString">20BN-something-something V2</span></span></a></td><td class="Table__CellText"><span class="SemanticStringArray"><span class="SemanticString">The new release features:
Greatly increased number of videos
Object annotations and captioning
……</span></span></td><td class="Table__CellText"><span class="SemanticStringArray"><span class="SemanticString">19.4GB</span></span></td><td class="Table__CellText"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://20bn.com/datasets/something-something/v2">https://20bn.com/datasets/something-something/v2</a></span></span></td><td class="Table__CellText"><span class="SemanticStringArray"></span></td><td class="Table__CellText"><span class="SemanticStringArray"><span class="SemanticString">69.18%(rgb+flow)
67.71%(</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">TSM ResNet-101</strong></span><span class="SemanticString">, RGB+Flow)
56.24%(</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">2-stream TRN</strong></span><span class="SemanticString"> (ECCV&#x27;18 update))</span></span></td></tr><tr><td class="Table__CellTitle"><a href="https://www.notion.so/0f02f7d5d2a84aca90e9e9247976de35"><span class="SemanticStringArray"><span class="SemanticString">Kinetics</span></span></a></td><td class="Table__CellText"><span class="SemanticStringArray"><span class="SemanticString">A large-scale, high-quality dataset of URL links to approximately 650,000 video clips that covers 700 human action classes, including </span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">human-object interactions</strong></span><span class="SemanticString"> such as playing instruments, as well as </span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">human-human interactions</strong></span><span class="SemanticString"> such as shaking hands and hugging. Each action class has at least 600 video clips. Each clip is human annotated with a single action class and lasts around 10s.</span></span></td><td class="Table__CellText"><span class="SemanticStringArray"></span></td><td class="Table__CellText"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://deepmind.com/research/open-source/kinetics">https://deepmind.com/research/open-source/kinetics</a></span></span></td><td class="Table__CellText"><span class="SemanticStringArray"></span></td><td class="Table__CellText"><span class="SemanticStringArray"></span></td></tr><tr><td class="Table__CellTitle"><a href="https://www.notion.so/aaa1d110d99f4e0da57bf415aef6575f"><span class="SemanticStringArray"><span class="SemanticString">UCF101</span></span></a></td><td class="Table__CellText"><span class="SemanticStringArray"><span class="SemanticString">UCF101 is an action recognition data set of realistic action videos, collected from YouTube, having 101 action categories.
UCF101 gives the largest diversity in terms of actions and with the presence of</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold"> large variations in camera motion, object appearance and pose, object scale, viewpoint, cluttered background, illumination conditions</strong></span><span class="SemanticString">, etc
The action categories can be divided into five types: </span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">1)Human-Object Interaction 2) Body-Motion Only 3) Human-Human Interaction 4) Playing Musical Instruments 5) Sports.</strong></span><span class="SemanticString">
....</span></span></td><td class="Table__CellText"><span class="SemanticStringArray"><span class="SemanticString">around 6.5GB</span></span></td><td class="Table__CellText"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://www.crcv.ucf.edu/data/UCF101.php">https://www.crcv.ucf.edu/data/UCF101.php</a></span></span></td><td class="Table__CellText"><span class="SemanticStringArray"></span></td><td class="Table__CellText"><span class="SemanticStringArray"><span class="SemanticString">98%(Two-Stream I3D)
95.9%(</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">TSM</strong></span><span class="SemanticString">)</span></span></td></tr><tr><td class="Table__CellTitle"><a href="https://www.notion.so/4697860c65294bb7b666e6ea4b7ea1fd"><span class="SemanticStringArray"><span class="SemanticString">HMDB51</span></span></a></td><td class="Table__CellText"><span class="SemanticStringArray"><span class="SemanticString">HMDB collected from various sources, mostly from movies, and a small proportion from public databases such as the Prelinger archive, YouTube and Google videos. The dataset contains 6849 clips divided into 51 action categories.
The actions categories can be grouped in five types:
</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">General facial actions</strong></span><span class="SemanticString"> smile, laugh, chew, talk.
</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Facial actions with object manipulation</strong></span><span class="SemanticString">: smoke, eat, drink.
</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">General body movements</strong></span><span class="SemanticString">: cartwheel, clap hands...
</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Body movements with object interaction</strong></span><span class="SemanticString">: brush hair, catch, draw sword...
</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Body movements for human interaction</strong></span><span class="SemanticString">: fencing, hug...
......</span></span></td><td class="Table__CellText"><span class="SemanticStringArray"><span class="SemanticString">around 2GB</span></span></td><td class="Table__CellText"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/">https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/</a></span></span></td><td class="Table__CellText"><span class="SemanticStringArray"></span></td><td class="Table__CellText"><span class="SemanticStringArray"><span class="SemanticString">80.9%(Two-Stream I3D)
73.5%(</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">TSM</strong></span><span class="SemanticString">)</span></span></td></tr><tr><td class="Table__CellTitle"><a href="https://www.notion.so/50589d7c136841e3be829414a0a341c9"><span class="SemanticStringArray"><span class="SemanticString">Multi-Moments in Time</span></span></a></td><td class="Table__CellText"><span class="SemanticStringArray"><span class="SemanticString">From Multi-Moments in Time Challenge 2019, the goal of this challenge is to detect the event labels depicted in a 3 second video.
Moments in Time dataset includes multiple action labels per video. Labels in the dataset can pertain to actions recognizable using one or more of the audio/visual streams (i.e. audio actions, visual actions, or audio-visual actions)</span></span></td><td class="Table__CellText"><span class="SemanticStringArray"></span></td><td class="Table__CellText"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="http://moments.csail.mit.edu/challenge_iccv_2019.html">http://moments.csail.mit.edu/challenge_iccv_2019.html</a></span></span></td><td class="Table__CellText"><span class="SemanticStringArray"><span class="SemanticString">动作主体可以是人，动物，物体乃至自然现象
数据集的类内差异和类间差异均很大
存在部分或完全依赖于声音信息的动作，如clapping</span></span></td><td class="Table__CellText"><span class="SemanticStringArray"><span class="SemanticString">63.84%(ResNext + VisText + Only Audio)</span></span></td></tr><tr><td class="Table__CellTitle"><a href="https://www.notion.so/beee2ab53a6f4b46b141205c2abc9644"><span class="SemanticStringArray"><span class="SemanticString">20BN-jester V1</span></span></a></td><td class="Table__CellText"><span class="SemanticStringArray"><span class="SemanticString">a large collection of densely-labeled video clips that show humans performing pre-definded hand gestures in front of a laptop camera or webcam</span></span></td><td class="Table__CellText"><span class="SemanticStringArray"><span class="SemanticString">22.8GB</span></span></td><td class="Table__CellText"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://20bn.com/datasets/jester">https://20bn.com/datasets/jester</a></span></span></td><td class="Table__CellText"><span class="SemanticStringArray"></span></td><td class="Table__CellText"><span class="SemanticStringArray"><span class="SemanticString">97.26%(MobileNet+NL+SlowFast)
97.09%(Fusion_TSN_LSTM)
94.78%(</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">TRN</strong></span><span class="SemanticString"> (CVPR&#x27;18 submission))</span></span></td></tr><tr><td class="Table__CellTitle"><a href="https://www.notion.so/f190ababc54d428f87ebf6b02ce088c6"><span class="SemanticStringArray"><span class="SemanticString">ActivityNet</span></span></a></td><td class="Table__CellText"><span class="SemanticStringArray"><span class="SemanticString">aims at covering a wide range of complex human activities that are of interest to people in their daily living. 
illustrate three scenarios in which ActivityNet can be used to compare algorithms for human activity understanding: global video classification,trimmed activity classification and activity detection.</span></span></td><td class="Table__CellText"><span class="SemanticStringArray"></span></td><td class="Table__CellText"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="http://activity-net.org/index.html">http://activity-net.org/index.html</a></span></span></td><td class="Table__CellText"><span class="SemanticStringArray"></span></td><td class="Table__CellText"><span class="SemanticStringArray"><span class="SemanticString">90.2%(</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">TSN)</strong></span></span></td></tr><tr><td class="Table__CellTitle"><a href="https://www.notion.so/f238b9bd03be49368ccac14fbc0a7984"><span class="SemanticStringArray"><span class="SemanticString">Charades</span></span></a></td><td class="Table__CellText"><span class="SemanticStringArray"><span class="SemanticString">Charades is dataset composed of 9848 videos of daily indoors activities collected through Amazon Mechanical Turk. 267 different users were presented with a sentence, that includes objects and actions from a fixed vocabulary, and they recorded a video acting out the sentence (like in a game of Charades). The dataset contains 66,500 temporal annotations for 157 action classes, 41,104 labels for 46 object classes, and 27,847 textual descriptions of the videos. This work was presented at ECCV2016.</span></span></td><td class="Table__CellText"><span class="SemanticStringArray"><span class="SemanticString">13GB,..,76GB</span></span></td><td class="Table__CellText"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://prior.allenai.org/projects/charades">https://prior.allenai.org/projects/charades</a></span></span></td><td class="Table__CellText"><span class="SemanticStringArray"><span class="SemanticString">既包含video-level的分类，又包含frame-level的分类</span></span></td><td class="Table__CellText"><span class="SemanticStringArray"></span></td></tr><tr><td class="Table__CellTitle"><a href="https://www.notion.so/0421c97f42dc421a9fc92c89402db53b"><span class="SemanticStringArray"><span class="SemanticString">THUMOS 14</span></span></a></td><td class="Table__CellText"><span class="SemanticStringArray"><span class="SemanticString">THUMOS 2014 will conduct the challenge on temporally untrimmed videos. The participants may train their methods using trimmed clips but will be required to test their systems on untrimmed data.
All videos are collected from YouTube</span></span></td><td class="Table__CellText"><span class="SemanticStringArray"></span></td><td class="Table__CellText"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://www.crcv.ucf.edu/THUMOS14/home.html">https://www.crcv.ucf.edu/THUMOS14/home.html</a></span></span></td><td class="Table__CellText"><span class="SemanticStringArray"></span></td><td class="Table__CellText"><span class="SemanticStringArray"></span></td></tr></tbody></table></div></div><div id="https://www.notion.so/b0174f99d621436da163fd4e257b7f02" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/7125828a7e464209a8f35f0ccf84a8e0" class="CollectionInline"><h3><a class="Anchor" href="#https://www.notion.so/7125828a7e464209a8f35f0ccf84a8e0"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">时域依赖与否划分</span></span></h3><div class="Table"><table><thead><tr><th style="width:276px">DataSets</th><th>Temporal Dependency</th></tr></thead><tbody><tr><td class="Table__CellTitle"><a href="https://www.notion.so/a06243db374145609d3d598adf2091f1"><span class="SemanticStringArray"><span class="SemanticString">UCF, Kinetics, Moments</span></span></a></td><td class="Table__CellText"><span class="SemanticStringArray"><span class="SemanticString">No</span></span></td></tr><tr><td class="Table__CellTitle"><a href="https://www.notion.so/1100b08de76b412e9511c3d36946be37"><span class="SemanticStringArray"><span class="SemanticString">Something, Jester, Charades</span></span></a></td><td class="Table__CellText"><span class="SemanticStringArray"><span class="SemanticString">Yes</span></span></td></tr></tbody></table></div></div><h2 id="https://www.notion.so/24a14f34b4294d9999493bc180e22de9" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/24a14f34b4294d9999493bc180e22de9"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">参考</span></span></h2><div id="https://www.notion.so/ea9862d381f34192adec028dbf3c6015" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://zhuanlan.zhihu.com/p/86461157">https://zhuanlan.zhihu.com/p/86461157</a></span></span></p></div><div id="https://www.notion.so/634e8b1676064dfa87406ca3f99efc00" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="http://www.aas.net.cn/fileZDHXB/journal/article/zdhxb/2018/6/PDF/zdhxb-44-6-978.pdf">http://www.aas.net.cn/fileZDHXB/journal/article/zdhxb/2018/6/PDF/zdhxb-44-6-978.pdf</a></span></span></p></div><div id="https://www.notion.so/989ba8b40c5240388e309156af9d5f93" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://zhuanlan.zhihu.com/p/48262145">https://zhuanlan.zhihu.com/p/48262145</a></span></span></p></div><div id="https://www.notion.so/a939acbc7ecc4e4daedc74dc0d23e4bd" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://blog.csdn.net/wzmsltw/article/details/78915585">https://blog.csdn.net/wzmsltw/article/details/78915585</a></span></span></p></div><div id="https://www.notion.so/9fa6ffa772904fe882cd0038fbcbe181" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div></article>  <div id="vcomments"></div>
  <script>
    new Valine({
      el: '#vcomments',
      appId: 'x3yNlOp8snKVbHLuM3MDgyPm-gzGzoHsz',
      appKey: 'IxbPmtJ4C181kqwCPyUfHb67',
      avatar: 'retro',
      placeholder: 'Welcome to comment here233!',
      requiredFields: ['nick', 'mail']
    })
  </script>
  <footer class="Footer">
    <div>&copy; idzc.me 2020</div>
    <div>&centerdot;</div>
    <div>Powered by <a href="https://github.com/dragonman225/notablog" target="_blank"
        rel="noopener noreferrer">Notablog</a>.
    </div>
  </footer>
</body>

</html>